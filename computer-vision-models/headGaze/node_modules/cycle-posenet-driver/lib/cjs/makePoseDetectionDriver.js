"use strict";
var __assign = (this && this.__assign) || Object.assign || function(t) {
    for (var s, i = 1, n = arguments.length; i < n; i++) {
        s = arguments[i];
        for (var p in s) if (Object.prototype.hasOwnProperty.call(s, p))
            t[p] = s[p];
    }
    return t;
};
var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {
    return new (P || (P = Promise))(function (resolve, reject) {
        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }
        function rejected(value) { try { step(generator["throw"](value)); } catch (e) { reject(e); } }
        function step(result) { result.done ? resolve(result.value) : new P(function (resolve) { resolve(result.value); }).then(fulfilled, rejected); }
        step((generator = generator.apply(thisArg, _arguments || [])).next());
    });
};
var __generator = (this && this.__generator) || function (thisArg, body) {
    var _ = { label: 0, sent: function() { if (t[0] & 1) throw t[1]; return t[1]; }, trys: [], ops: [] }, f, y, t, g;
    return g = { next: verb(0), "throw": verb(1), "return": verb(2) }, typeof Symbol === "function" && (g[Symbol.iterator] = function() { return this; }), g;
    function verb(n) { return function (v) { return step([n, v]); }; }
    function step(op) {
        if (f) throw new TypeError("Generator is already executing.");
        while (_) try {
            if (f = 1, y && (t = y[op[0] & 2 ? "return" : op[0] ? "throw" : "next"]) && !(t = t.call(y, op[1])).done) return t;
            if (y = 0, t) op = [0, t.value];
            switch (op[0]) {
                case 0: case 1: t = op; break;
                case 4: _.label++; return { value: op[1], done: false };
                case 5: _.label++; y = op[1]; op = [0]; continue;
                case 7: op = _.ops.pop(); _.trys.pop(); continue;
                default:
                    if (!(t = _.trys, t = t.length > 0 && t[t.length - 1]) && (op[0] === 6 || op[0] === 2)) { _ = 0; continue; }
                    if (op[0] === 3 && (!t || (op[1] > t[0] && op[1] < t[3]))) { _.label = op[1]; break; }
                    if (op[0] === 6 && _.label < t[1]) { _.label = t[1]; t = op; break; }
                    if (t && _.label < t[2]) { _.label = t[2]; _.ops.push(op); break; }
                    if (t[2]) _.ops.pop();
                    _.trys.pop(); continue;
            }
            op = body.call(thisArg, _);
        } catch (e) { op = [6, e]; y = 0; } finally { f = t = 0; }
        if (op[0] & 5) throw op[1]; return { value: op[0] ? op[1] : void 0, done: true };
    }
};
var __importDefault = (this && this.__importDefault) || function (mod) {
    return (mod && mod.__esModule) ? mod : { "default": mod };
};
var __importStar = (this && this.__importStar) || function (mod) {
    if (mod && mod.__esModule) return mod;
    var result = {};
    if (mod != null) for (var k in mod) if (Object.hasOwnProperty.call(mod, k)) result[k] = mod[k];
    result["default"] = mod;
    return result;
};
Object.defineProperty(exports, "__esModule", { value: true });
var snabbdom_pragma_1 = __importDefault(require("snabbdom-pragma"));
var xstream_1 = __importDefault(require("xstream"));
var adapt_1 = require("@cycle/run/lib/adapt");
var dat_gui_1 = __importDefault(require("dat.gui"));
var stats_js_1 = __importDefault(require("stats.js"));
var posenet = __importStar(require("@tensorflow-models/posenet"));
var utils_1 = require("./utils");
// adapted from
//   https://github.com/tensorflow/tfjs-models/blob/fc0a80d8ddbd2845fca4a61355dc5c54d1b43e0d/posenet/demos/camera.js#L102-L182
// Sets up dat.gui controller on the top-right of the window
function setupGui(cameras, net, guiState) {
    guiState.net = net;
    if (cameras.length > 0) {
        guiState.camera = cameras[0].deviceId;
    }
    var gui = new dat_gui_1.default.GUI({ width: 300, autoPlace: false });
    // The single-pose algorithm is faster and simpler but requires only one
    // person to be in the frame or results will be innaccurate. Multi-pose works
    // for more than 1 person
    var algorithmController = gui.add(guiState, 'algorithm', ['single-pose', 'multi-pose']);
    // The input parameters have the most effect on accuracy and speed of the
    // network
    var input = gui.addFolder('Input');
    // Architecture: there are a few PoseNet models varying in size and
    // accuracy. 1.01 is the largest, but will be the slowest. 0.50 is the
    // fastest, but least accurate.
    var architectureController = input.add(guiState.input, 'mobileNetArchitecture', ['1.01', '1.00', '0.75', '0.50']);
    // Output stride:  Internally, this parameter affects the height and width of
    // the layers in the neural network. The lower the value of the output stride
    // the higher the accuracy but slower the speed, the higher the value the
    // faster the speed but lower the accuracy.
    input.add(guiState.input, 'outputStride', [8, 16, 32]);
    // Image scale factor: What to scale the image by before feeding it through
    // the network.
    input.add(guiState.input, 'imageScaleFactor').min(0.2).max(1.0);
    input.open();
    // Pose confidence: the overall confidence in the estimation of a person's
    // pose (i.e. a person detected in a frame)
    // Min part confidence: the confidence that a particular estimated keypoint
    // position is accurate (i.e. the elbow's position)
    var single = gui.addFolder('Single Pose Detection');
    single.add(guiState.singlePoseDetection, 'minPoseConfidence', 0.0, 1.0);
    single.add(guiState.singlePoseDetection, 'minPartConfidence', 0.0, 1.0);
    single.open();
    var multi = gui.addFolder('Multi Pose Detection');
    multi.add(guiState.multiPoseDetection, 'maxPoseDetections')
        .min(1)
        .max(20)
        .step(1);
    multi.add(guiState.multiPoseDetection, 'minPoseConfidence', 0.0, 1.0);
    multi.add(guiState.multiPoseDetection, 'minPartConfidence', 0.0, 1.0);
    // nms Radius: controls the minimum distance between poses that are returned
    // defaults to 20, which is probably fine for most use cases
    multi.add(guiState.multiPoseDetection, 'nmsRadius').min(0.0).max(40.0);
    var output = gui.addFolder('Output');
    output.add(guiState.output, 'showVideo');
    output.add(guiState.output, 'showSkeleton');
    output.add(guiState.output, 'showPoints');
    output.open();
    architectureController.onChange(function (architecture) {
        guiState.changeToArchitecture = architecture;
    });
    algorithmController.onChange(function (value) {
        switch (guiState.algorithm) {
            case 'single-pose':
                multi.close();
                single.open();
                break;
            case 'multi-pose':
                single.close();
                multi.open();
                break;
        }
    });
    return gui;
}
/**
 * [PoseNet](https://github.com/tensorflow/tfjs-models/tree/master/posenet)
 * driver factory.
 *
 * @param options possible key includes
 *
 *   * videoWidth {number} An optional video height (default: 640).
 *   * videoWidth {number} An optional video width (default: 480).
 *   * flipHorizontal {boolean} An optional flag for horizontally flipping the
 *     video (default: true).
 *
 * @return {Driver} the PoseNet Cycle.js driver function. It takes a stream
 *   of [`PoseNetParameters`](./src/pose_detection.tsx) and returns a stream of
 *   [`Pose` arrays](https://github.com/tensorflow/tfjs-models/tree/master/posenet#via-npm).
 *
 */
function makePoseDetectionDriver(_a) {
    var _b = _a === void 0 ? {} : _a, _c = _b.videoWidth, videoWidth = _c === void 0 ? 640 : _c, _d = _b.videoHeight, videoHeight = _d === void 0 ? 480 : _d, _e = _b.flipHorizontal, flipHorizontal = _e === void 0 ? true : _e;
    var id = String(Math.random()).substr(2);
    var divID = "posenet-" + id;
    var videoID = "pose-video-" + id;
    var canvasID = "pose-canvas-" + id;
    return function (params$) {
        var _this = this;
        var params = null;
        var initialParams = {
            algorithm: 'single-pose',
            input: {
                mobileNetArchitecture: utils_1.isMobile() ? '0.50' : '0.75',
                outputStride: 16,
                imageScaleFactor: utils_1.isMobile() ? 0.2 : 0.5,
            },
            singlePoseDetection: {
                minPoseConfidence: 0.2,
                minPartConfidence: 0.5,
            },
            multiPoseDetection: {
                maxPoseDetections: 5,
                minPoseConfidence: 0.15,
                minPartConfidence: 0.1,
                nmsRadius: 30.0,
            },
            output: {
                showVideo: true,
                showSkeleton: true,
                showPoints: true,
            },
            net: null,
            changeToArchitecture: null,
            fps: utils_1.isMobile() ? 5 : 10,
            stopRequested: false,
        };
        params$.fold(function (prev, params) {
            Object.keys(params).map(function (key) {
                if (typeof params[key] === 'object') {
                    Object.assign(prev[key], params[key]);
                }
                else {
                    prev[key] = params[key];
                }
                return prev;
            });
            return prev;
        }, initialParams).addListener({
            next: function (newParams) {
                params = newParams;
            }
        });
        function poseDetectionFrame(params, video, context, callback) {
            return __awaiter(this, void 0, void 0, function () {
                var _a, imageScaleFactor, outputStride, poses, minPoseConfidence, minPartConfidence, _b, pose, outPoses;
                return __generator(this, function (_c) {
                    switch (_c.label) {
                        case 0:
                            if (!params.changeToArchitecture) return [3 /*break*/, 2];
                            // Important to purge variables and free up GPU memory
                            params.net.dispose();
                            // Load the PoseNet model weights for either the 0.50, 0.75, 1.00, or
                            // 1.01 version
                            _a = params;
                            return [4 /*yield*/, posenet.load(+params.changeToArchitecture)];
                        case 1:
                            // Load the PoseNet model weights for either the 0.50, 0.75, 1.00, or
                            // 1.01 version
                            _a.net = _c.sent();
                            params.changeToArchitecture = null;
                            _c.label = 2;
                        case 2:
                            imageScaleFactor = params.input.imageScaleFactor;
                            outputStride = +params.input.outputStride;
                            poses = [];
                            _b = params.algorithm;
                            switch (_b) {
                                case 'single-pose': return [3 /*break*/, 3];
                                case 'multi-pose': return [3 /*break*/, 5];
                            }
                            return [3 /*break*/, 7];
                        case 3: return [4 /*yield*/, params.net.estimateSinglePose(video, imageScaleFactor, flipHorizontal, outputStride)];
                        case 4:
                            pose = _c.sent();
                            poses.push(pose);
                            minPoseConfidence = +params.singlePoseDetection.minPoseConfidence;
                            minPartConfidence = +params.singlePoseDetection.minPartConfidence;
                            return [3 /*break*/, 7];
                        case 5: return [4 /*yield*/, params.net.estimateMultiplePoses(video, imageScaleFactor, flipHorizontal, outputStride, params.multiPoseDetection.maxPoseDetections, params.multiPoseDetection.minPartConfidence, params.multiPoseDetection.nmsRadius)];
                        case 6:
                            poses = _c.sent();
                            minPoseConfidence = +params.multiPoseDetection.minPoseConfidence;
                            minPartConfidence = +params.multiPoseDetection.minPartConfidence;
                            return [3 /*break*/, 7];
                        case 7:
                            context.clearRect(0, 0, videoWidth, videoHeight);
                            if (params.output.showVideo) {
                                context.save();
                                context.scale(-1, 1);
                                context.translate(-videoWidth, 0);
                                context.drawImage(video, 0, 0, videoWidth, videoHeight);
                                context.restore();
                            }
                            // For each pose (i.e. person) detected in an image, loop through the
                            // poses and draw the resulting skeleton and keypoints if over certain
                            // confidence scores
                            poses.forEach(function (_a) {
                                var score = _a.score, keypoints = _a.keypoints;
                                if (score >= minPoseConfidence) {
                                    if (params.output.showPoints) {
                                        utils_1.drawKeypoints(keypoints, minPartConfidence, context);
                                    }
                                    if (params.output.showSkeleton) {
                                        utils_1.drawSkeleton(keypoints, minPartConfidence, context);
                                    }
                                }
                            });
                            outPoses = poses
                                .filter(function (pose) { return pose.score >= minPoseConfidence; })
                                .map(function (pose) { return (__assign({}, pose, { keypoints: pose.keypoints.filter(function (keypoint) { return keypoint.score >= minPartConfidence; }) })); });
                            if (callback) {
                                callback(outPoses);
                            }
                            return [2 /*return*/];
                    }
                });
            });
        }
        var timeoutId = {};
        var poses$ = xstream_1.default.create({
            start: function (listener) {
                // Poll the canvas element
                var intervalID = setInterval(function () { return __awaiter(_this, void 0, void 0, function () {
                    var _this = this;
                    var video, canvas, context, _a, stats, interval, start, execute, gui;
                    return __generator(this, function (_b) {
                        switch (_b.label) {
                            case 0:
                                if (!document.querySelector("#" + canvasID)) {
                                    console.debug("Waiting for #" + canvasID + " to appear...");
                                    return [2 /*return*/];
                                }
                                clearInterval(intervalID);
                                return [4 /*yield*/, utils_1.setupCamera(document.querySelector("#" + videoID), videoWidth, videoHeight)];
                            case 1:
                                video = _b.sent();
                                video.play();
                                canvas = document.querySelector("#" + canvasID);
                                context = canvas.getContext('2d');
                                canvas.width = videoWidth;
                                canvas.height = videoHeight;
                                // Setup the posenet
                                _a = params;
                                return [4 /*yield*/, posenet.load(0.75)];
                            case 2:
                                // Setup the posenet
                                _a.net = _b.sent();
                                stats = new stats_js_1.default();
                                interval = 1000 / params.fps;
                                start = Date.now();
                                execute = function () { return __awaiter(_this, void 0, void 0, function () {
                                    var elapsed;
                                    return __generator(this, function (_a) {
                                        switch (_a.label) {
                                            case 0:
                                                elapsed = Date.now() - start;
                                                if (!(elapsed > interval)) return [3 /*break*/, 2];
                                                stats.begin();
                                                start = Date.now();
                                                return [4 /*yield*/, poseDetectionFrame(params, video, context, listener.next.bind(listener))];
                                            case 1:
                                                _a.sent();
                                                stats.end();
                                                if (!timeoutId)
                                                    return [2 /*return*/];
                                                timeoutId = setTimeout(execute, 0);
                                                return [3 /*break*/, 3];
                                            case 2:
                                                if (!timeoutId)
                                                    return [2 /*return*/];
                                                this._timeoutId = setTimeout(execute, interval - elapsed);
                                                _a.label = 3;
                                            case 3: return [2 /*return*/];
                                        }
                                    });
                                }); };
                                execute();
                                // Setup UIs
                                stats.showPanel(0);
                                stats.dom.style.setProperty('position', 'absolute');
                                document.querySelector("#" + divID).appendChild(stats.dom);
                                gui = setupGui(video, params.net, params);
                                gui.domElement.style.setProperty('position', 'absolute');
                                gui.domElement.style.setProperty('top', '0px');
                                gui.domElement.style.setProperty('right', '0px');
                                document.querySelector("#" + divID)
                                    .appendChild(gui.domElement);
                                gui.closed = true;
                                return [2 /*return*/];
                        }
                    });
                }); }, 1000);
            },
            stop: function () {
                timeoutId = null;
            },
        });
        var vdom$ = xstream_1.default.of((snabbdom_pragma_1.default.createElement("div", { id: divID, style: { position: "relative" } },
            snabbdom_pragma_1.default.createElement("video", { id: videoID, style: { display: 'none' }, autoPlay: true }),
            snabbdom_pragma_1.default.createElement("canvas", { id: canvasID }))));
        return {
            DOM: adapt_1.adapt(vdom$),
            poses: adapt_1.adapt(poses$),
        };
    };
}
exports.makePoseDetectionDriver = makePoseDetectionDriver;
//# sourceMappingURL=makePoseDetectionDriver.js.map